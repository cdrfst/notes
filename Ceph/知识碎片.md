[引用自官方](https://cephdocs.readthedocs.io/en/stable/rados/configuration/common/#osds)

```shell
ssh {new-osd-host}
sudo mkfs -t {fstype} /dev/{disk}
sudo mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
```

We recommend using the `xfs` file system when running **mkfs**. (`btrfs` and `ext4` are not recommended and are no longer tested.)

[日志大小设置](https://cephdocs.readthedocs.io/en/stable/rados/configuration/osd-config-ref/)
When using Filestore, the journal size should be at least twice the product of the expected drive speed multiplied by `filestore_max_sync_interval`. However, the most common practice is to partition the journal drive (often an SSD), and mount it such that Ceph uses the entire partition for the journal.

[BlueStore可使用逻辑卷或GPT分区](https://cephdocs.readthedocs.io/en/stable/rados/configuration/bluestore-config-ref/)
`--data` can be a Logical Volume using _vg/lv_ notation. Other devices can be existing logical volumes or GPT partitions.

[OSD配置](https://cephdocs.readthedocs.io/en/stable/rados/configuration/bluestore-config-ref/)


#必备工具
sgdisk  (yum install -y gdisk)
时间同步到秒


ceph mon dump
ceph mon stat


#调优
scrub 参数



存储后端
BlueStore(default)

FileStore
FileStore必须指定journal.size  #https://cephdocs.readthedocs.io/en/stable/rados/configuration/common/#osds

Ceph production clusters typically deploy Ceph OSD Daemons where one node has one OSD daemon running a Filestore on one storage device. The BlueStore back end is now default, but when using Filestore you specify a journal size. For example:

[osd]
osd_journal_size = 10000

[osd.0]
host = {hostname} #manual deployments only.